\chapter{Security Assurance and Vulnerability Assessment}
\label{ch:security-assurance}


\section{Context and Objectives}
In a dependability-oriented project, security assurance is treated as part of operational reliability: defects that expose secrets, pull vulnerable libraries, or leak runtime details can quickly become availability incidents (e.g., compromised credentials, exploit-driven downtime, or denial-of-service amplification). The goal of this chapter is therefore not to claim ``perfect security'', but to document a concrete, traceable hardening process applied to the \textit{2Chance} codebase.

Following the laboratory guidance, we focused on three complementary assessment dimensions:
\begin{itemize}
  \item \textbf{Secret detection} at repository level (prevent accidental credential exposure);
  \item \textbf{Software Composition Analysis (SCA)} for vulnerable dependencies (supply-chain risk);
  \item \textbf{Static Application Security Testing (SAST)} for risky code patterns (defensive coding at entry points).
\end{itemize}

The work was conducted iteratively with an explicit evidence trail: \textit{scan} $\rightarrow$ \textit{triage} $\rightarrow$ \textit{fix} $\rightarrow$ \textit{re-scan}. All outputs, notes, and before/after artifacts are stored under \path{reports/evidence/security/}.

\section{Process, Toolchain, and Evidence Trail}
Security analysis was executed with the following toolchain:
\begin{itemize}
  \item GitGuardian (\texttt{ggshield}) for repository secret scanning;
  \item Snyk (CLI) for Maven dependency vulnerability scanning;
  \item SonarQube Cloud for static analysis on Java servlets (controller layer).
\end{itemize}

Evidence is organized per tool:
\begin{itemize}
  \item \path{reports/evidence/security/gitguardian/} (scan outputs, fixes, walkthrough);
  \item \path{reports/evidence/security/snyk/} (baseline scan, post-fix scan, dependency changes);
  \item \path{reports/evidence/security/sonar/} (CSV/JSON issue exports and remediation walkthrough).
\end{itemize}

\subsection{Before/After Summary}
Table~\ref{tab:security-summary} summarizes the before/after state as captured by the three tools. Numbers are taken from the baseline reports and the final verification runs stored in the evidence folders.

\begin{table}[h]
\centering
\caption{Security assessment summary (before/after) based on stored evidence.}
\label{tab:security-summary}
\begin{tabular}{l c c l}
\hline
\textbf{Tool} & \textbf{Before} & \textbf{After} & \textbf{Outcome} \\
\hline
GitGuardian & 6 occurrences (Generic Password) & 0 & Resolved \\
Snyk & 14 issues total & 0 & Resolved \\
SonarQube Cloud & 96 issues (\texttt{java:S1989}) & 0 & Resolved \\
\hline
\end{tabular}
\end{table}

\section{Security Mechanisms in CI/CD (Checklist Item 0)}
The repository includes a GitHub Actions workflow (\path{.github/workflows/maven.yml}) that builds the project and runs tests with a MySQL service. The pipeline also builds and pushes a Docker image using Docker Hub credentials provided through GitHub Secrets (e.g., \texttt{DOCKER\_USERNAME}, \texttt{DOCKER\_PASSWORD}). This is a relevant security mechanism because it keeps deployment credentials out of the codebase and prevents accidental exposure through commits.

At the time of writing, GitGuardian/Snyk/Sonar scans are executed as part of the documented assessment process and stored as evidence, but they are not yet enforced as mandatory CI ``quality gates''.
As an optional improvement (and to strengthen checklist compliance), the existing workflow can be extended with lightweight security jobs (e.g., \texttt{ggshield} repo scan, \texttt{snyk test}, or a SonarCloud analysis step). Since this affects the CI configuration, we treat it as a recommended enhancement rather than a claim of already-enforced gating.

\section{GitGuardian: Secret Detection and Remediation}
\subsection{Why Secret Scanning}
Secrets committed to a repository have a disproportionate blast radius: a single leaked password or token may enable unauthorized access and bypass many other controls. Secret scanning aims to prevent this class of incident early, before credentials are reused or propagated across forks and clones.

\subsection{Baseline Findings}
The baseline GitGuardian repository scan detected \textbf{two instances} of the \emph{Generic Password} class, for a total of \textbf{six occurrences} distributed across two files:
\begin{itemize}
  \item \path{src/prompts/03\_unit\_testing\_implementation.txt} (documentation content);
  \item \path{src/test/java/services/RegistrazioneServiceTest.java} (unit tests).
\end{itemize}

Raw evidence is stored in:
\begin{itemize}
  \item \path{reports/evidence/security/gitguardian/ggshield\_repo\_scan.txt} (baseline);
  \item \path{reports/evidence/security/gitguardian/ggshield\_repo\_scan\_after\_cleanup.txt} (post-fix).
\end{itemize}

\subsection{Remediation and Rationale}
The remediation strategy was conservative and practical:
\begin{itemize}
  \item Replace password-like strings in documentation with clearly non-secret placeholders;
  \item Ensure test credentials are treated as \emph{test-only} values and not presented as realistic secrets.
\end{itemize}

To reduce noise and prevent repeated false positives during future scans, a repository-level configuration was added:
\begin{itemize}
  \item \path{.gitguardian.yaml} excludes \path{src/prompts/**} and test paths (e.g., \path{src/test/**});
  \item It also ignores a small set of known test password patterns (\texttt{Pass123!}, \texttt{validPassword}, \texttt{TEST\_PASSWORD}).
\end{itemize}

This configuration is a trade-off: it improves signal-to-noise ratio, but it must remain narrow and justified to avoid masking real exposures. The applied changes and rationale are documented in:
\path{reports/evidence/security/gitguardian/security\_analysis\_gitguardian.md} and
\path{reports/evidence/security/gitguardian/gitguardian\_fixes.md}.

\subsection{Verification}
After cleanup, GitGuardian reported \textbf{0} detected secrets in the repository scan, as recorded in \path{ggshield\_repo\_scan\_after\_cleanup.txt}.

\section{Snyk: Dependency (SCA) Vulnerability Assessment}
\subsection{Why SCA}
Modern applications inherit a significant portion of their attack surface from third-party libraries. Software Composition Analysis focuses on known vulnerabilities (e.g., CVEs) in direct and transitive dependencies. The typical remediation is to upgrade to a patched version or migrate to a maintained artifact coordinate.

\subsection{Baseline Findings}
The baseline scan output (\path{reports/evidence/security/snyk/snyk\_test.txt}) reports \textbf{11 vulnerable paths} at first pass, which were consolidated in the analysis notes as \textbf{14 issues total} across remediation phases. The affected components clustered around:
\begin{itemize}
  \item \texttt{commons-io:commons-io} (resource exhaustion);
  \item \texttt{org.json:json} (DoS-related issues);
  \item MySQL connector and its transitive chain;
  \item JSTL (\texttt{javax.servlet:jstl}) (XXE and legacy namespace).
\end{itemize}

\subsection{Remediation}
Two kinds of fixes were applied:
\begin{itemize}
  \item \textbf{Version upgrades} for maintained artifacts (e.g., \texttt{commons-io} from \texttt{2.10.0} to \texttt{2.14.0}, \texttt{org.json:json} from \texttt{20210307} to \texttt{20231013});
  \item \textbf{Artifact migration} where the old coordinate was deprecated or unpatched:
  \begin{itemize}
    \item \texttt{mysql:mysql-connector-java} $\rightarrow$ \texttt{com.mysql:mysql-connector-j} (\texttt{9.5.0});
    \item \texttt{javax.servlet:jstl} $\rightarrow$ Apache Taglibs (\texttt{org.apache.taglibs}, \texttt{1.2.5}).
  \end{itemize}
\end{itemize}

The exact changes and their rationale are documented in:
\path{reports/evidence/security/snyk/security\_analysis\_snyk.md} and
\path{reports/evidence/security/snyk/snyk\_fixes.md}.

\subsection{Verification}
A final scan (\path{reports/evidence/security/snyk/snyk\_test\_after\_cleanup.txt}) reports \textbf{0 issues}. In addition, a full build and test run was executed to reduce the risk of dependency-related regressions (see the Snyk fixes notes; test suite result: 533 tests passed).

\section{SonarQube Cloud: Static Security Analysis on the Controller Layer}
\subsection{Why SAST for Servlets}
Static analysis is particularly useful at application entry points: in servlet-based architectures, controller methods (\texttt{doGet}, \texttt{doPost}, lifecycle hooks) are a common source of uncontrolled exception propagation and accidental information leakage. Even when the underlying bug is ``only'' an exception, the security impact is often practical: stack traces and internal error details can reveal system structure and enable targeted attacks.

\subsection{Baseline Findings}
SonarQube Cloud reported \textbf{96 issues} mapped to the rule \textbf{\texttt{java:S1989}} (``Exceptions should not be thrown by servlet methods''). Raw exports are stored in:
\begin{itemize}
  \item \path{reports/evidence/security/sonar/sonar\_security\_issues.csv} (initial);
  \item \path{reports/evidence/security/sonar/sonar\_security\_issues\_2.csv} (mid-remediation);
  \item \path{reports/evidence/security/sonar/sonar\_security\_issues\_final.csv} (final).
\end{itemize}

\subsection{Remediation Pattern and Motivation}
The remediation follows a consistent pattern: servlet entry points were refactored to prevent checked and runtime exceptions from escaping to the container. In practice this means:
\begin{itemize}
  \item wrap controller logic in \texttt{try-catch} blocks;
  \item convert failures into controlled HTTP responses without exposing internal details;
  \item avoid repetitive boilerplate through a small shared utility.
\end{itemize}

A notable implementation detail is the introduction of \texttt{src/main/java/utils/ResponseUtils.java}, which centralizes safe error response handling (e.g., \texttt{ResponseUtils.sendError(response, status, message)}). This reduces duplication and also mitigates secondary exception paths caused by \texttt{response.sendError()} (which can throw \texttt{IOException}). The full walkthrough and file list are documented in:
\path{reports/evidence/security/sonar/walkthrough\_sonar.md} and \path{reports/evidence/security/sonar/sonar\_fixes.md}.

\subsection{Verification}
The final Sonar exports report \textbf{0 issues}. A Maven build and full test suite execution were performed after the refactor (533/533 tests passed), providing additional confidence that changes were behavior-preserving.

\section{Web Application Shows No Vulnerabilities (Checklist Item 2)}
Static tools and dependency scanners reduce risk, but they are not sufficient to claim that the \emph{running} web application ``shows no vulnerabilities''. This checklist item should be addressed with a final, system-level verification step (dynamic scanning and/or targeted manual checks) against a deployed instance of the application.

\paragraph{Planned verification approach.}
A reasonable baseline is a DAST-style scan (e.g., OWASP ZAP baseline) against the deployed endpoints, combined with a short manual sanity check focused on the most relevant risks for servlet/JSP applications (authentication flows, input handling, session management, error pages). The outcome should be documented as raw output plus a short interpretation, stored under a dedicated evidence folder, for example:
\begin{center}
\path{reports/evidence/security/dast/}
\end{center}

\paragraph{Status.}
\textit{[Insert the final DAST / manual verification evidence and summary here. Until the final check is executed, this section must be considered incomplete.]}

\section{Threats to Validity}
\paragraph{False positives and rule interpretation.}
All three tools may produce false positives or findings that require contextual interpretation. GitGuardian findings on ``generic passwords'' are particularly sensitive to documentation and tests; Snyk reports may change as advisories evolve; Sonar rules may flag patterns that are benign in specific contexts. For this reason, human triage is an explicit part of the documented process.

\paragraph{Environment and reproducibility.}
Tool results depend on execution context: local configurations, authentication state for cloud dashboards, and repository history. To mitigate this, baseline and post-fix outputs were archived in the repository. Still, exact outputs may differ across time due to updated vulnerability databases (Snyk) or updated analyzer rules (Sonar).

\paragraph{Dependency upgrades and behavioral drift.}
Upgrading dependencies improves security posture but may introduce subtle compatibility changes. The project mitigated this risk by rebuilding and running the full test suite after remediation. Residual behavioral drift is still possible and should be monitored in future changes.

\paragraph{Coverage gaps without dynamic testing.}
Without the final checklist item 2 verification, the assessment remains incomplete at the runtime layer. Static and SCA results should be interpreted as ``reduced risk'' rather than ``no vulnerabilities'' for the deployed application.

\section{Conclusions and Next Steps}
The security assurance work followed a concrete scan--fix--verify loop and produced repository-stored evidence for each tool. The project removed:
(i) secret-like patterns from documentation/tests and constrained false positives via \path{.gitguardian.yaml},
(ii) dependency vulnerabilities through upgrades and targeted artifact migrations,
and (iii) risky servlet exception propagation patterns by introducing consistent controller-side error handling and a shared \texttt{ResponseUtils} utility.

Immediate next steps are:
\begin{itemize}
  \item complete checklist item 2 with a final dynamic/system-level verification and store raw evidence;
  \item (optional) extend the CI workflow with lightweight security jobs to enforce scanning as part of the pipeline rather than as a manual step.
\end{itemize}

\section{Missing Data}
To finalize this chapter with maximum auditability, the following details would be useful:
\begin{itemize}
  \item Whether security scans are already integrated in CI/CD beyond build/test (and if yes, which jobs and triggers);
  \item The exact procedure and tool chosen for checklist item 2 (DAST/manual), plus raw outputs to archive;
  \item How SonarQube Cloud analysis is triggered in your setup (manual dashboard run vs CI integration), so the execution method can be documented precisely.
\end{itemize}
