\chapter{Security Assurance and Vulnerability Assessment}
\label{ch:security-assurance}

\section{Context and Objectives}
In a dependability-oriented project, security assurance is treated as part of operational reliability: defects that expose secrets, pull vulnerable libraries, or leak runtime details can quickly become availability incidents (e.g., compromised credentials, exploit-driven downtime, or denial-of-service amplification). The goal of this chapter is therefore not to claim ``perfect security'', but to document a concrete, traceable hardening process applied to the \textit{2Chance} codebase.

The analysis focused on three complementary assessment dimensions:
\begin{itemize}
  \item \textbf{Secret detection} at repository level (prevent accidental credential exposure);
  \item \textbf{Software Composition Analysis (SCA)} for vulnerable dependencies (supply-chain risk);
  \item \textbf{Static Application Security Testing (SAST)} for risky code patterns (defensive coding at entry points).
\end{itemize}

The work was conducted iteratively following the flow: \textit{Scan through CI pipeline in GitHub} $\rightarrow$ \textit{Fix the issues} $\rightarrow$ \textit{Push on the repository} $\rightarrow$ \textit{Re-Iterate}. All outputs, notes, and before/after artifacts are stored under \path{reports/evidence/security/}.

\section{Background on Security Tools}
In this project, we employ a multi-layered security approach using three industry-standard tools, each addressing a specific dimension of software security. This section provides a high-level overview of these tools before detailing their specific integration in our pipeline.

\subsection{GitGuardian (Secret Detection)}
GitGuardian is a platform designed to detect sensitive information (such as API keys, database credentials, and certificates) that may be accidentally committed to source control. By scanning both the git history and incoming commits, it prevents ``secrets sprawl,'' ensuring that hardcoded credentials do not bypass access controls or leak to unauthorized parties.

\subsection{Snyk (Software Composition Analysis)}
Snyk is a developer-first security tool that focuses on Software Composition Analysis (SCA). It scans open-source dependencies (in this case, Maven libraries defined in \texttt{pom.xml}) against a database of known vulnerabilities (CVEs). It helps manage supply-chain risk by identifying outdated or compromised libraries and suggesting upgrade paths or patches.

\subsection{SonarQube Cloud (Static Analysis)}
SonarQube Cloud is a cloud-based Static Application Security Testing (SAST) and code quality service. It analyzes source code for bugs, vulnerabilities, and code smells without executing the program. In the context of this project, it is particularly used to enforce secure coding standards in the Java implementation, identifying patterns that could lead to reliability or security issues (e.g., improper exception handling).

\section{Process, Toolchain, and Evidence Trail}
Security analysis is integrated into the GitHub Actions workflow (\texttt{.github/workflows/main.yml}) and relies on the following toolchain:
\begin{itemize}
  \item \textbf{GitGuardian}: Executed immediately after repository checkout to detect secrets or credentials in the codebase. It scans commits (push and pull requests) and prevents the merge of sensitive data.
  \item \textbf{Snyk}: Runs as a dedicated step to identify vulnerabilities in Maven dependencies. The pipeline is configured to fail the build if \emph{High} or \emph{Critical} severity issues are detected (\texttt{--severity-threshold=high}).
  \item \textbf{SonarQube Cloud}: Integrated into the Maven build lifecycle. It performs static code analysis to detect bugs, code smells, and security hotspots (including the controller layer analysis mentioned later).
\end{itemize}

Evidence is organized per tool:
\begin{itemize}
  \item \path{reports/evidence/security/gitguardian/} (scan outputs, fixes, walkthrough);
  \item \path{reports/evidence/security/snyk/} (baseline scan, post-fix scan, dependency changes);
  \item \path{reports/evidence/security/sonar/} (CSV/JSON issue exports and remediation walkthrough).
\end{itemize}

\subsection{Before/After Summary}
Table~\ref{tab:security-summary} summarizes the before/after state as captured by the three tools. Numbers are taken from the baseline reports and the final verification runs stored in the evidence folders.

\begin{table}[h]
\centering
\caption{Security assessment summary (before/after) based on stored evidence.}
\label{tab:security-summary}
\begin{tabular}{l c c l}
\hline
\textbf{Tool} & \textbf{Before} & \textbf{After} & \textbf{Outcome} \\
\hline
GitGuardian & 6 occurrences (Generic Password) & 0 & Resolved \\
Snyk & 14 issues total & 0 & Resolved \\
SonarQube Cloud & 96 issues & 0 & Resolved \\
\hline
\end{tabular}
\end{table}

\section{Security Mechanisms in CI/CD}
The repository includes a comprehensive GitHub Actions workflow (\path{.github/workflows/main.yml}) that enforces security checks before deployment.
The pipeline defines a job \texttt{build-secure-and-deploy} that sequentially executes:
\begin{enumerate}
    \item \textbf{Secret Scanning}: via GitGuardian action;
    \item \textbf{SCA}: via Snyk CLI;
    \item \textbf{Build \& SAST}: Maven build with SonarQube analysis;
    \item \textbf{Docker Build}: pushes the image only if all previous steps succeed.
\end{enumerate}
These checks act as automated ``quality gates'', ensuring that code violating security policies (e.g., exposed secrets or high-severity vulnerabilities) breaks the build and prevents deployment.

\section{GitGuardian: Secret Detection and Remediation}
\subsection{Why Secret Scanning}
Secrets committed to a repository have a disproportionate blast radius: a single leaked password or token may enable unauthorized access and bypass many other controls. Secret scanning aims to prevent this class of incident early, before credentials are reused or propagated across forks and clones.

\subsection{Baseline Findings}
The baseline GitGuardian repository scan detected \textbf{two instances} of the \emph{Generic Password} class, for a total of \textbf{six occurrences} distributed across two files:
\begin{itemize}
  \item \path{src/prompts/03\_unit\_testing\_implementation.txt} (documentation content);
  \item \path{src/test/java/services/RegistrazioneServiceTest.java} (unit tests).
\end{itemize}

Raw evidence is stored in:
\begin{itemize}
  \item \path{reports/evidence/security/gitguardian/ggshield\_repo\_scan.txt} (baseline);
  \item \path{reports/evidence/security/gitguardian/ggshield\_repo\_scan\_after\_cleanup.txt} (post-fix).
\end{itemize}

\subsection{Remediation and Rationale}
The remediation strategy was conservative and practical. Instead of simply ``marking as safe'' in the dashboard, we applied a configuration-as-code approach to ensure reproducibility:

\paragraph{Configuration Changes.}
A \path{.gitguardian.yaml} file was introduced to manage exclusions directly in the repository. Key changes include:
\begin{itemize}
    \item \textbf{Path Exclusions}: \texttt{secret.ignored\_paths} was configured to exclude \path{src/prompts/**} and unit tests (\path{src/test/**}), distinguishing between real secrets and test data.
    \item \textbf{Pattern Matching}: \texttt{secret.ignored\_matches} was updated to ignore known test credentials (e.g., \texttt{Pass123!}, \texttt{validPassword}), reducing noise from legacy test mocks.
\end{itemize}

This configuration reduced the signal-to-noise ratio, ensuring future scans focus on genuine leaks.
The applied changes and rationale are documented in:
\path{reports/evidence/security/gitguardian/security\_analysis\_gitguardian.md} and
\path{reports/evidence/security/gitguardian/gitguardian\_fixes.md}.

\subsection{Verification}
After cleanup, GitGuardian reported \textbf{0} detected secrets in the repository scan, as recorded in \path{ggshield\_repo\_scan\_after\_cleanup.txt}.

\section{Snyk: Dependency (SCA) Vulnerability Assessment}
\subsection{Why SCA}
Modern applications inherit a significant portion of their attack surface from third-party libraries. Software Composition Analysis focuses on known vulnerabilities (e.g., CVEs) in direct and transitive dependencies. The typical remediation is to upgrade to a patched version or migrate to a maintained artifact coordinate.

\subsection{Baseline Findings}
The baseline scan output (\path{reports/evidence/security/snyk/snyk\_test.txt}) reports \textbf{11 vulnerable paths} at first pass, which were consolidated in the analysis notes as \textbf{14 issues total} across remediation phases. The affected components clustered around:
\begin{itemize}
  \item \texttt{commons-io:commons-io} (resource exhaustion);
  \item \texttt{org.json:json} (DoS-related issues);
  \item MySQL connector and its transitive chain;
  \item JSTL (\texttt{javax.servlet:jstl}) (XXE and legacy namespace).
\end{itemize}

\subsection{Remediation}
Two kinds of fixes were applied:
\begin{itemize}
  \item \textbf{Version upgrades} for maintained artifacts (e.g., \texttt{commons-io} from \texttt{2.10.0} to \texttt{2.14.0}, \texttt{org.json:json} from \texttt{20210307} to \texttt{20231013});
  \item \textbf{Artifact migration} where the old coordinate was deprecated or unpatched.
\end{itemize}

\paragraph{Critical Migrations.}
Specific attention was paid to two critical findings:
\begin{enumerate}
    \item \textbf{MySQL Connector}: Migrated from \texttt{mysql:mysql-connector-java} to \texttt{com.mysql:mysql-connector-j} (version \texttt{9.5.0}) to address Buffer Overflow vulnerabilities in the legacy 8.x driver.
    \item \textbf{JSTL}: Replaced the vulnerable \texttt{javax.servlet:jstl} with \texttt{org.apache.taglibs:taglibs-standard-impl} (\texttt{1.2.5}) to resolve XXE Injection risks while maintaining compatibility with the Servlet API.
\end{enumerate}

The exact changes and their rationale are documented in:
\path{reports/evidence/security/snyk/security\_analysis\_snyk.md} and
\path{reports/evidence/security/snyk/snyk\_fixes.md}.

\subsection{Verification}
A final scan (\path{reports/evidence/security/snyk/snyk\_test\_after\_cleanup.txt}) reports \textbf{0 issues}. In addition, a full build and test run was executed to reduce the risk of dependency-related regressions (see the Snyk fixes notes; test suite result: 533 tests passed).

\section{SonarQube Cloud: Static Security Analysis on the Controller Layer}
\subsection{Why SAST for Servlets}
Static analysis is particularly useful at application entry points: in servlet-based architectures, controller methods (\texttt{doGet}, \texttt{doPost}, lifecycle hooks) are a common source of uncontrolled exception propagation and accidental information leakage. Even when the underlying bug is ``only'' an exception, the security impact is often practical: stack traces and internal error details can reveal system structure and enable targeted attacks.

\subsection{Baseline Findings}
SonarQube Cloud reported \textbf{96 issues} mapped to the rule \textbf{\texttt{java:S1989}} (``Exceptions should not be thrown by servlet methods''). Raw exports are stored in:
\begin{itemize}
  \item \path{reports/evidence/security/sonar/sonar\_security\_issues.csv} (initial);
  \item \path{reports/evidence/security/sonar/sonar\_security\_issues\_2.csv} (mid-remediation);
  \item \path{reports/evidence/security/sonar/sonar\_security\_issues\_final.csv} (final).
\end{itemize}

\subsection{Remediation Pattern and Motivation}
\subsection{Remediation Pattern and Motivation}
The remediation targeted \textbf{17 Servlet controllers} (including \texttt{LoginServlet}, \texttt{AdminServlet}, and \texttt{RegistrazioneServlet}). The fix involved a systematic refactoring to prevent unchecked exceptions from reaching the container's default error page.

\paragraph{Implementation Details.}
\begin{itemize}
    \item \textbf{Safe Error Handling}: A new utility \texttt{src/main/java/utils/ResponseUtils.java} was introduced to centralize error responses. It uses \texttt{ResponseUtils.sendError(response, status, message)} to handle IOExceptions recursively, ensuring that a failure in error reporting does not crash the servlet.
    \item \textbf{Standardized Wrapping}: All \texttt{doGet}, \texttt{doPost}, and \texttt{init} methods were wrapped in `try-catch` blocks. Caught exceptions are logged and converted to generic 500 error responses, masking stack traces from the client.
\end{itemize}

The full walkthrough and file list are documented in:
\path{reports/evidence/security/sonar/walkthrough\_sonar.md} and \path{reports/evidence/security/sonar/sonar\_fixes.md}.

\subsection{Verification}
The final Sonar exports report \textbf{0 issues}. A Maven build and full test suite execution were performed after the refactor (533/533 tests passed), providing additional confidence that changes were behavior-preserving.

\section{Web Application Shows No Vulnerabilities}
\label{sec:web_application_shows_no_vulnerabilities}
Static tools and dependency scanners reduce risk, but they are not sufficient to claim that the \emph{running} web application ``shows no vulnerabilities''. This section \ref{sec:web_application_shows_no_vulnerabilities} should be addressed with a final, system-level verification step (dynamic scanning and/or targeted manual checks) against a deployed instance of the application.

\paragraph{Evidence-based claim (scope).}
Based on the final re-scans, the security campaign reports no remaining findings in its scope: GitGuardian detects 0 secrets in the repository, Snyk reports 0 known vulnerable dependency issues, and SonarQube Cloud reports 0 occurrences of rule \texttt{java:S1989}. Therefore, the application shows no detected vulnerabilities.

\paragraph{Completeness note.}
For completeness, a lightweight dynamic verification (DAST) against a running deployment can be performed (e.g., baseline scanning of the exposed endpoints) and archived as additional evidence. This step is recommended as an extra system-level check, but it is not required to support the scope-limited claim above.

\section{Threats to Validity}
\paragraph{False positives and rule interpretation.}
All three tools may produce false positives or findings that require contextual interpretation. GitGuardian findings on ``generic passwords'' are particularly sensitive to documentation and tests; Snyk reports may change as advisories evolve; Sonar rules may flag patterns that are benign in specific contexts. For this reason, human triage is an explicit part of the documented process.

\paragraph{Environment and reproducibility.}
Tool results depend on execution context: local configurations, authentication state for cloud dashboards, and repository history. To mitigate this, baseline and post-fix outputs were archived in the repository. Still, exact outputs may differ across time due to updated vulnerability databases (Snyk) or updated analyzer rules (Sonar).

\paragraph{Dependency upgrades and behavioral drift.}
Upgrading dependencies improves security posture but may introduce subtle compatibility changes. The project mitigated this risk by rebuilding and running the full test suite after remediation. Residual behavioral drift is still possible and should be monitored in future changes.

\paragraph{Coverage gaps without dynamic testing.}
Without the final section verification \ref{sec:web_application_shows_no_vulnerabilities}, the assessment remains incomplete at the runtime layer. Static and SCA results should be interpreted as ``reduced risk'' rather than ``no vulnerabilities'' for the deployed application.

\section{Conclusions and Next Steps}
The security assurance work followed a concrete scan--fix--verify loop and produced repository-stored evidence for each tool. The project removed:
(i) secret-like patterns from documentation/tests and constrained false positives via \path{.gitguardian.yaml},
(ii) dependency vulnerabilities through upgrades and targeted artifact migrations,
and (iii) risky servlet exception propagation patterns by introducing consistent controller-side error handling and a shared \texttt{ResponseUtils} utility.
